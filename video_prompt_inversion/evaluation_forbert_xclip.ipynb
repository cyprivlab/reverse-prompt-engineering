{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import argparse\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    \"\"\"Load JSON file and return as Python object\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def extract_video_name(path):\n",
    "    \"\"\"Extract video filename without extension from path\"\"\"\n",
    "    return os.path.splitext(os.path.basename(path))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_path = 'output/batch_video_results.json'\n",
    "references_path = 'datasets/video_prompts.json'\n",
    "#output_path = 'output/batch_video_results.csv'\n",
    "\n",
    "\n",
    "# Load JSON files\n",
    "predictions_data = load_json_file(prediction_path)\n",
    "references_data = load_json_file(references_path)\n",
    "\n",
    "if not predictions_data or not references_data:\n",
    "    print(\"Failed to load one or both JSON files. Exiting.\")\n",
    "\n",
    "# Create a dictionary of reference prompts keyed by video name\n",
    "reference_dict = {}\n",
    "for item in references_data:\n",
    "    # Adapt this according to your reference JSON structure\n",
    "    if isinstance(item, dict) and 'video' in item and 'conversations' in item:\n",
    "        reference_dict[item['video']] = item['conversations'][1]['value']\n",
    "\n",
    "\n",
    "# Create a list to store all comparison data\n",
    "comparison_data = []\n",
    "\n",
    "# Process each prediction\n",
    "for pred in predictions_data:\n",
    "    video_name = pred['video_name']\n",
    "    prediction = pred.get('prediction', '')\n",
    "    \n",
    "    # Find matching reference\n",
    "    reference = reference_dict.get(video_name, '')\n",
    "    \n",
    "    \n",
    "    # Add to comparison data\n",
    "    comparison_data.append({\n",
    "        'video_name': video_name,\n",
    "        'reference': reference,\n",
    "        'prediction': prediction,\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(comparison_data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save to CSV\n",
    "df.to_parquet('output/compare_evalutaion_results.parquet', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result_df = pd.read_parquet('output/compare_evalutaion_results.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bert_score import score\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def Calmetic(references:list[list[str]], predictions:list[str]):\n",
    "    '''\n",
    "    Input format:\n",
    "\n",
    "    predictions = [\n",
    "        \"What is the capital of France?\",\n",
    "        \"Who wrote the book?\",\n",
    "        \"What is the largest planet?\"\n",
    "    ]\n",
    "\n",
    "    references = [\n",
    "        [\"What is the capital city of France?\"],\n",
    "        [\"Who is the author of the book?\"],\n",
    "        [\"Which planet is the largest in the solar system?\"]\n",
    "    ]\n",
    "    '''\n",
    "\n",
    "    # # 加载 BLEU 评分器\n",
    "    # bleu_metric = load_metric(\"bleu\")\n",
    "\n",
    "    # # 计算 BLEU 分数\n",
    "    predictions_tokenized = [word_tokenize(pred) for pred in predictions]\n",
    "    references_tokenized = [[word_tokenize(refs[0])] for refs in references]\n",
    "    # B_S = {}\n",
    "    # for n in range(1, 5):\n",
    "    #     bleu_metric.add_batch(predictions=predictions_tokenized, references=references_tokenized)\n",
    "    #     results = bleu_metric.compute(max_order=n)\n",
    "    #     B_S[f\"BLEU-{n}\"] = results\n",
    "    bleu_metric = evaluate.load(\"bleu\")\n",
    "    B_S = bleu_metric.compute(predictions=predictions, references=references,tokenizer=word_tokenize)\n",
    "    for i,n in enumerate(B_S['precisions']):\n",
    "        print(f\"BLEU-{i+1} score: {n:.5f}\")\n",
    "        \n",
    "\n",
    "\n",
    "    # 加载 ROUGE 评分器\n",
    "    rouge_metric = load_metric(\"rouge\")\n",
    "    '''\n",
    "    ROUGE-1: 衡量生成文本和参考文本之间的 unigram 匹配。\n",
    "    ROUGE-2: 衡量生成文本和参考文本之间的 bigram 匹配。\n",
    "    ROUGE-L: 衡量生成文本和参考文本之间的最长公共子序列(LCS)。\n",
    "    ROUGE-Lsum: 基于 LCS 的一个变体，专门用于长文本的评估。\n",
    "    '''\n",
    "    # 计算 ROUGE 分数\n",
    "    rouge_results = rouge_metric.compute(predictions=predictions, references=references)\n",
    "    rouge1_mid_f1 = rouge_results['rouge1'][1][2]\n",
    "    rouge2_mid_f1 = rouge_results['rouge2'][1][2]\n",
    "    rougeL_mid_f1 = rouge_results['rougeL'][1][2]\n",
    "    rougeLsum_mid_f1 = rouge_results['rougeLsum'][1][2]\n",
    "    print(f\"ROUGE-1 F1 score: {rouge1_mid_f1:.5f}\")\n",
    "    print(f\"ROUGE-2 F1 score: {rouge2_mid_f1:.5f}\")\n",
    "    print(f\"ROUGE-L F1 score: {rougeL_mid_f1:.5f}\")\n",
    "    print(f\"ROUGE-Lsum F1 score: {rougeLsum_mid_f1:.5f}\")\n",
    "\n",
    "    # 计算 METEOR 分数\n",
    "    meteor_scores = [meteor_score(references=refs, hypothesis=pred) for pred, refs in zip(predictions_tokenized, references_tokenized)]\n",
    "    average_meteor_score = sum(meteor_scores) / len(meteor_scores)\n",
    "    print(f\"Average METEOR score: {average_meteor_score:.5f}\")\n",
    "\n",
    "    # 计算 BERTScore 分数\n",
    "\n",
    "    P, R, F1 = score(predictions, [ref[0] for ref in references], lang=\"en\", verbose=False)\n",
    "    average_bert_score = F1.mean().item()\n",
    "    print(f\"Average BERTScore F1: {average_bert_score:.5f}\")\n",
    "\n",
    "    return {\n",
    "        \"BLEU\":B_S,\n",
    "        \"ROUGE\":rouge_results,\n",
    "        \"METERO\":meteor_scores,\n",
    "        \"BERTScore\":{\"Precision\":P,\"Recall\":R,\"F1\":F1},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = [ [i] for i in result_df['reference']]\n",
    "content = result_df['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_sentence(s):\n",
    "    s = s.replace(\"，\", \",\")  # 替换中文逗号\n",
    "    s = re.sub(r\"-[a-z]+\\s*\\d+\", \"\", s)  # 去掉如 -fps 24 这种参数\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "predictions = [clean_sentence(p) for p in result_df['prediction']]\n",
    "references = [[clean_sentence(r)] for r in result_df['reference']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = Calmetic(references=references,predictions=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两个重要数值\n",
    "print(res['BLEU'])\n",
    "print(res['ROUGE']['rougeLsum'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity as torch_cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')  #SentenceTransformer(\"bert-base-uncased\") \n",
    "\n",
    "reference_texts_ = [ clean_sentence(i) for i in result_df['reference'] ]\n",
    "embeddings1 = sentence_model.encode(predictions, convert_to_tensor=True)\n",
    "embeddings2 = sentence_model.encode(result_df['reference'], convert_to_tensor=True)\n",
    "\n",
    "cosine_scores_2 = util.pytorch_cos_sim(embeddings1, embeddings2)  \n",
    "\n",
    "# 输出余弦相似度的值\n",
    "print(f\"Average Cosine Similarity: {cosine_scores_2.diagonal().mean()}\")\n",
    "print(f\"Biggest Cosine Similarity: {cosine_scores_2.diagonal().max()}\")\n",
    "print(f\"Middle Cosine Similarity: {cosine_scores_2.diagonal().median()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 获取余弦相似度数据\n",
    "cos_sim_scores = cosine_scores_2.diagonal()\n",
    "\n",
    "# 创建直方图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(cos_sim_scores.cpu().numpy(), bins=50, edgecolor='black')\n",
    "plt.title('Distribution of Cosine Similarity Scores')\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices where cosine similarity > 0.7\n",
    "high_sim_indices = (cos_sim_scores > 0.6).nonzero().squeeze().tolist()\n",
    "\n",
    "# Create a new dataframe with high similarity pairs\n",
    "high_sim_df = pd.DataFrame({\n",
    "    'video_name': [result_df['video_name'][i] for i in high_sim_indices],\n",
    "    'reference': [result_df['reference'][i] for i in high_sim_indices],\n",
    "    'prediction': [result_df['prediction'][i] for i in high_sim_indices],\n",
    "    'similarity_score': cos_sim_scores[high_sim_indices].cpu().numpy()\n",
    "})\n",
    "\n",
    "# Sort by similarity score in descending order\n",
    "high_sim_df = high_sim_df.sort_values('similarity_score', ascending=False)\n",
    "\n",
    "print(f\"Number of pairs with similarity > 0.7: {len(high_sim_df)}\")\n",
    "display(high_sim_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_sim_df.to_parquet('output/high_sim_over0.6_df.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_sentence(s):\n",
    "    s = s.replace(\"，\", \",\")  # 替换中文逗号\n",
    "    s = re.sub(r\"-[a-z]+\\s*\\d+\", \"\", s)  # 去掉如 -fps 24 这种参数\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "predictions = [p for p in high_sim_df['prediction']]\n",
    "references = [[r] for r in high_sim_df['reference']]\n",
    "\n",
    "res = Calmetic(references=references,predictions=predictions)\n",
    "print(res['BLEU'])\n",
    "print(res['ROUGE']['rougeLsum'][1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity as torch_cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')  #SentenceTransformer(\"bert-base-uncased\") \n",
    "\n",
    "embeddings1 = sentence_model.encode(high_sim_df['prediction'], convert_to_tensor=True)\n",
    "embeddings2 = sentence_model.encode(high_sim_df['reference'], convert_to_tensor=True)\n",
    "\n",
    "cosine_scores_2 = util.pytorch_cos_sim(embeddings1, embeddings2)  \n",
    "\n",
    "# 输出余弦相似度的值\n",
    "print(f\"Average Cosine Similarity: {cosine_scores_2.diagonal().mean()}\")\n",
    "print(f\"Biggest Cosine Similarity: {cosine_scores_2.diagonal().max()}\")\n",
    "print(f\"Middle Cosine Similarity: {cosine_scores_2.diagonal().median()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    '''\n",
    "    Sample a given number of frame indices from the video.\n",
    "    Args:\n",
    "        clip_len (`int`): Total number of frames to sample.\n",
    "        frame_sample_rate (`int`): Sample every n-th frame.\n",
    "        seg_len (`int`): Maximum allowed index of sample's last frame.\n",
    "    Returns:\n",
    "        indices (`List[int]`): List of sampled frame indices\n",
    "    '''\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices\n",
    "\n",
    "\n",
    "# video clip consists of 300 frames (10 seconds at 30 FPS)\n",
    "file_path = hf_hub_download(\n",
    "    repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n",
    ")\n",
    "container = av.open(file_path)\n",
    "\n",
    "# sample 8 frames\n",
    "indices = sample_frame_indices(clip_len=8, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n",
    "video = read_video_pyav(container, indices)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/xclip-base-patch32\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/xclip-base-patch32\")\n",
    "\n",
    "inputs = processor(\n",
    "    text=[\"playing sports\", \"eating spaghetti\", \"go shopping\"],\n",
    "    videos=list(video),\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ")\n",
    "\n",
    "# forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits_per_video = outputs.logits_per_video  # this is the video-text similarity score\n",
    "probs = logits_per_video.softmax(dim=1)  # we can take the softmax to get the label probabilities\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation For Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "compare_promts = pd.read_parquet('output/prompt_vid_cos_res/DI+FT_Video2Text_RL_generation_vidsim&bert_over0.5_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_promts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "\n",
    "def bert_sim_eval(candidate_sentence, target_sentence):\n",
    "    # 将句子放入列表中，因为 bert-score 的 score 函数期望列表输入\n",
    "    candidates = [candidate_sentence]\n",
    "    references = [target_sentence]\n",
    "\n",
    "    # 计算 BertScore\n",
    "    P, R, F1 = score(candidates, references, lang='en', verbose=True, model_type='bert-base-uncased')\n",
    "    return {\"P\":P.item(),\"R\":R.item(),\"F1\":F1.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "P_ours = []\n",
    "R_ours = []\n",
    "F1_ours = []\n",
    "for index,item in tqdm(compare_promts.iterrows(),total=len(compare_promts)):    \n",
    "    res = bert_sim_eval(item['reference'],item['prediction'])\n",
    "    P_ours.append(res['P'])\n",
    "    R_ours.append(res['R'])\n",
    "    F1_ours.append(res['F1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(P_ours)/len(P_ours))\n",
    "print(sum(R_ours)/len(R_ours))\n",
    "print(sum(F1_ours)/len(F1_ours))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import CogVideoXPipeline\n",
    "from diffusers.utils import export_to_video\n",
    "import gc\n",
    "\n",
    "\n",
    "def gen_pic(compare_df):\n",
    "    pipe = CogVideoXPipeline.from_pretrained(\n",
    "        \"THUDM/CogVideoX-2b\",\n",
    "        torch_dtype=torch.float16\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # pipe.enable_model_cpu_offload()\n",
    "    # pipe.enable_sequential_cpu_offload()\n",
    "    pipe.vae.enable_slicing()\n",
    "    pipe.vae.enable_tiling()\n",
    "    \n",
    "    for idx,row in compare_df.iterrows():\n",
    "        prompt = row['RL_generation']\n",
    "        video_name = row['video_name']\n",
    "\n",
    "        print(f\"[{idx+1}/{len(compare_df)}] Generating: RL_gen_{video_name}\")\n",
    "\n",
    "        try:\n",
    "            video = pipe(\n",
    "                prompt=prompt,\n",
    "                num_videos_per_prompt=1,\n",
    "                num_inference_steps=50,\n",
    "                num_frames=49,\n",
    "                guidance_scale=6,\n",
    "                generator=torch.Generator(device=\"cuda\").manual_seed(42),\n",
    "            ).frames[0]\n",
    "\n",
    "            export_to_video(video, f\"output/RL_genvideos/RL_gen_{video_name}\", fps=8)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error generating {video_name}: {e}\")\n",
    "\n",
    "        finally:\n",
    "            # 显式释放显存\n",
    "            del video\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        # if 'pipe' in globals():\n",
    "        #     del pipe\n",
    "        #     gc.collect()\n",
    "        #     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_pic(compare_promts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "from huggingface_hub import hf_hub_download\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices\n",
    "\n",
    "def get_video_features(video_path, clip_len=8):\n",
    "    \"\"\"Extract features from a video file using XCLIP model\"\"\"\n",
    "    container = av.open(video_path)\n",
    "    indices = sample_frame_indices(clip_len=clip_len, frame_sample_rate=1, \n",
    "                                 seg_len=container.streams.video[0].frames)\n",
    "    video = read_video_pyav(container, indices)\n",
    "    \n",
    "    processor = AutoProcessor.from_pretrained(\"microsoft/xclip-base-patch32\")\n",
    "    model = AutoModel.from_pretrained(\"microsoft/xclip-base-patch32\")\n",
    "    \n",
    "    inputs = processor(videos=list(video), return_tensors=\"pt\")\n",
    "    video_features = model.get_video_features(**inputs)\n",
    "    return video_features\n",
    "\n",
    "def compute_video_similarity(video1_path, video2_path):\n",
    "    \"\"\"Compute cosine similarity between two videos\"\"\"\n",
    "    features1 = get_video_features(video1_path)\n",
    "    features2 = get_video_features(video2_path)\n",
    "    \n",
    "    # Compute cosine similarity between video features\n",
    "    similarity = cosine_similarity(features1, features2)\n",
    "    return similarity.item()\n",
    "\n",
    "def compare_videos(compare_df):\n",
    "    \"\"\"Compare original videos with generated videos\"\"\"\n",
    "    results = []\n",
    "    similarity_scores = []\n",
    "\n",
    "    for idx, row in tqdm(compare_df.iterrows(), total=len(compare_df)):\n",
    "        original_video = row['video_name']\n",
    "        generated_video = f\"RL_gen_{original_video}\"\n",
    "        \n",
    "        try:\n",
    "            similarity = compute_video_similarity(\n",
    "                f\"vidprom/cog_videos_example/{original_video}\",\n",
    "                f\"output/RL_genvideos/{generated_video}\"\n",
    "            )\n",
    "            \n",
    "            # Save intermediate results to JSON file\n",
    "            result = {\n",
    "                'video_name': original_video,\n",
    "                'generated_video': generated_video,\n",
    "                'reference': row['reference'],\n",
    "                'origin': row['Origin_generation'],\n",
    "                'DI_prediction': row['DI_generation'],\n",
    "                'video_similarity_score': similarity,\n",
    "                'promt_similarity_score': row['similarity_score']\n",
    "            }\n",
    "            \n",
    "            # Append to results list\n",
    "            results.append(result)\n",
    "            similarity_scores.append(similarity)\n",
    "\n",
    "            # Save current results to JSON file\n",
    "            with open('output/RL_gen_video_comparison_results.json', 'w') as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "                \n",
    "            print(f\"Processed {idx+1}/{len(compare_df)} videos. Current similarity: {similarity:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error comparing videos {original_video}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Create DataFrame with results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values('similarity_score', ascending=False)\n",
    "    \n",
    "    return results_df,similarity_scores\n",
    "\n",
    "# Example usage:\n",
    "# results = compare_videos(compare_df)\n",
    "# print(results.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results,video_cos_sim_scores = compare_videos(compare_promts)\n",
    "print(results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "vid_sim_res = pd.read_json('output/RL_gen_video_comparison_results.json')\n",
    "print(vid_sim_res.shape)\n",
    "vid_sim_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(compare_promts['video_name'] == vid_sim_res['video_name']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add RL_video_similarity_score column to compare_prompts using video_similarity_score from vid_sim_res\n",
    "compare_promts['RL_video_similarity_score'] = vid_sim_res['video_similarity_score']\n",
    "compare_promts['RL_generation'] = vid_sim_res['generated_video']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_promts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_promts.to_parquet('output/prompt_vid_cos_res/DI+FT_Video2Text_RL_generation_vidsim&bert_over0.5_df.parquet',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将vid_sim_res转换成datasets\n",
    "from datasets import Dataset\n",
    "vid_sim_res_ds = Dataset.from_pandas(vid_sim_res)\n",
    "vid_sim_res_ds.save_to_disk(\"output/prompt_vid_cos_res/video_comparison_results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "vid_sim_res_ds = datasets.load_from_disk(\"output/prompt_vid_cos_res/video_comparison_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_sim_res_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "similarity = compute_video_similarity(\"examples/video3.mp4\", \"examples/video3.mp4\")\n",
    "print(f\"Video similarity score: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "similarity = compute_video_similarity(\"examples/video3.mp4\", \"examples/video3.mp4\")\n",
    "print(f\"Video similarity score: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 综合DI和DI+FT的数据集，进行video_similarity的评估以及纯文本的评估\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DI_df = pd.read_parquet('output/prompt_vid_cos_res/video_comparison_results.parquet')\n",
    "DI_FT_df = pd.read_parquet('output/prompt_vid_cos_res/DI+FT_Video2Text_RL_generation_vidsim&bert_over0.5_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DI_FT_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DI_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将DI_FT_df中的对video_name在DI_df中找到video_similarity_score，并添加到DI_FT_df中\n",
    "DI_FT_df['DI_video_similarity_score'] = DI_FT_df['video_name'].apply(lambda x: DI_df[DI_df['video_name'] == x]['video_similarity_score'].values[0])\n",
    "DI_FT_df.head()\n",
    "#DI_FT_df.to_parquet('output/prompt_vid_cos_res/DI+FT_Video2Text_RL_generation_sim_over0.5_df.parquet',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "\n",
    "def bert_sim_eval(candidate_sentence, target_sentence):\n",
    "    # 将句子放入列表中，因为 bert-score 的 score 函数期望列表输入\n",
    "    candidates = [candidate_sentence]\n",
    "    references = [target_sentence]\n",
    "\n",
    "    # 计算 BertScore\n",
    "    P, R, F1 = score(candidates, references, lang='en', verbose=True)\n",
    "    return {\"P\":P.item(),\"R\":R.item(),\"F1\":F1.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "P_ours = []\n",
    "R_ours = []\n",
    "F1_ours = []\n",
    "for index,item in tqdm(DI_FT_df.iterrows(),total=len(DI_FT_df)):    \n",
    "    res = bert_sim_eval(item['DI_generation'],item['reference'])\n",
    "    P_ours.append(res['P'])\n",
    "    R_ours.append(res['R'])\n",
    "    F1_ours.append(res['F1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"P:\",sum(P_ours)/len(P_ours))\n",
    "print(\"R:\",sum(R_ours)/len(R_ours))\n",
    "print(\"F1:\",sum(F1_ours)/len(F1_ours))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将P_ours,R_ours,F1_ours添加到DI_FT_df中，只有一列，保存成列表，保证在读取的时候能识别成列表\n",
    "DI_FT_df['DI_bert_sim_score'] = DI_FT_df.apply(lambda x: [P_ours[x.name], R_ours[x.name], F1_ours[x.name]], axis=1)\n",
    "DI_FT_df.head()\n",
    "DI_FT_df.to_parquet('output/prompt_vid_cos_res/DI+FT_Video2Text_RL_generation_vidsim&bert_over0.5_df.parquet',index=False)\n",
    "\n",
    "\n",
    "# #读取DI_FT_df\n",
    "# DI_FT_df = pd.read_parquet('output/prompt_vid_cos_res/DI+FT_Video2Text_RL_generation_vidsim&bert_over0.5_df.parquet')\n",
    "# DI_FT_df.head()\n",
    "# #将DI_FT_df中的bert_sim_score转换成列表\n",
    "# DI_FT_df['RL_bert_sim_score'] = DI_FT_df['RL_bert_sim_score'].apply(lambda x: x[0])\n",
    "# DI_FT_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(P_ours)\n",
    "print(R_ours)\n",
    "print(F1_ours)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DI_FT_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the parquet file\n",
    "filtered_df = pd.read_parquet('output/prompt_vid_cos_res/DI+FT_Video2Text_RL_generation_sim_over0.6_df.parquet')\n",
    "\n",
    "# Get the video names from filtered_df\n",
    "filtered_video_names = filtered_df['video_name'].tolist()\n",
    "\n",
    "# Filter DI_FT_df to only include rows where video_name is in filtered_video_names\n",
    "filtered_DI_FT_df = DI_FT_df[DI_FT_df['video_name'].isin(filtered_video_names)]\n",
    "\n",
    "# Print number of rows in filtered dataframe\n",
    "print(f\"Number of rows in filtered dataframe: {len(filtered_DI_FT_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 假设 RL_bert_sim_score 每行是 [P, R, F1]\n",
    "scores = np.array(filtered_DI_FT_df['RL_bert_sim_score'].tolist())  # 变成二维数组，每列分别是P、R、F1\n",
    "\n",
    "P_avg = scores[:, 0].mean()\n",
    "R_avg = scores[:, 1].mean()\n",
    "F1_avg = scores[:, 2].mean()\n",
    "\n",
    "print(\"P:\", P_avg)\n",
    "print(\"R:\", R_avg)\n",
    "print(\"F1:\", F1_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_scores  = np.array(filtered_DI_FT_df['RL_video_similarity_score'].tolist())\n",
    "print(vid_scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogvid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
