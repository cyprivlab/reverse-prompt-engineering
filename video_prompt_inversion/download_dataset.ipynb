{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fd74ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from longvu.builder import load_pretrained_model\n",
    "from longvu.constants import (\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    IMAGE_TOKEN_INDEX,\n",
    ")\n",
    "from longvu.conversation import conv_templates, SeparatorStyle\n",
    "from longvu.mm_datautils import (\n",
    "    KeywordsStoppingCriteria,\n",
    "    process_images,\n",
    "    tokenizer_image_token,\n",
    ")\n",
    "from decord import cpu, VideoReader\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    \"./checkpoints/LongVU_Llama3_2_3B\", None, \"cambrian_llama\",load_8bit=True\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "video_path = \"./examples/video2.mp4\"\n",
    "qs = \"What's the prompt to generate the video?\"\n",
    "\n",
    "vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n",
    "fps = float(vr.get_avg_fps())\n",
    "frame_indices = np.array([i for i in range(0, len(vr), round(fps),)])\n",
    "video = []\n",
    "for frame_index in frame_indices:\n",
    "    img = vr[frame_index].asnumpy()\n",
    "    video.append(img)\n",
    "video = np.stack(video)\n",
    "image_sizes = [video[0].shape[:2]]\n",
    "video = process_images(video, image_processor, model.config)\n",
    "video = [item.unsqueeze(0) for item in video]\n",
    "\n",
    "qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\n",
    "conv = conv_templates[\"llama3\"].copy()\n",
    "conv.append_message(conv.roles[0], qs)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "\n",
    "input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(model.device)\n",
    "stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "keywords = [stop_str]\n",
    "stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n",
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        images=video,\n",
    "        image_sizes=image_sizes,\n",
    "        do_sample=False,\n",
    "        temperature=0.2,\n",
    "        max_new_tokens=128,\n",
    "        use_cache=True,\n",
    "        stopping_criteria=[stopping_criteria],\n",
    "    )\n",
    "pred = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "print(f\"\\nAnswer: {pred}\") # 4bit -> 11G GPU ; 8 bit -> 12G GPU 视频长度如果是5秒，则需要9G GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b380b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from longvu.builder import load_pretrained_model\n",
    "from longvu.constants import (\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    IMAGE_TOKEN_INDEX,\n",
    ")\n",
    "from longvu.conversation import conv_templates, SeparatorStyle\n",
    "from longvu.mm_datautils import (\n",
    "    KeywordsStoppingCriteria,\n",
    "    process_images,\n",
    "    tokenizer_image_token,\n",
    ")\n",
    "from decord import cpu, VideoReader\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    \"./checkpoints/LongVU_Qwen2_7B\", None, \"cambrian_qwen\",load_4bit=True\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "video_path = \"./examples/video1.mp4\"\n",
    "qs = \"Describe this video in detail\"\n",
    "\n",
    "vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n",
    "fps = float(vr.get_avg_fps())\n",
    "frame_indices = np.array([i for i in range(0, len(vr), round(fps),)])\n",
    "video = []\n",
    "for frame_index in frame_indices:\n",
    "    img = vr[frame_index].asnumpy()\n",
    "    video.append(img)\n",
    "video = np.stack(video)\n",
    "image_sizes = [video[0].shape[:2]]\n",
    "video = process_images(video, image_processor, model.config)\n",
    "video = [item.unsqueeze(0) for item in video]\n",
    "\n",
    "qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\n",
    "conv = conv_templates[\"qwen\"].copy()\n",
    "conv.append_message(conv.roles[0], qs)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "\n",
    "input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(model.device)\n",
    "stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "keywords = [stop_str]\n",
    "stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n",
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        images=video,\n",
    "        image_sizes=image_sizes,\n",
    "        do_sample=False,\n",
    "        temperature=0.2,\n",
    "        max_new_tokens=128,\n",
    "        use_cache=True,\n",
    "        stopping_criteria=[stopping_criteria],\n",
    "    )\n",
    "pred = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "print(f\"\\nAnswer: {pred}\") #17G GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8feefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from longvu.builder import load_pretrained_model\n",
    "from longvu.constants import (\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    IMAGE_TOKEN_INDEX,\n",
    ")\n",
    "from longvu.conversation import conv_templates, SeparatorStyle\n",
    "from longvu.mm_datautils import (\n",
    "    KeywordsStoppingCriteria,\n",
    "    process_images,\n",
    "    tokenizer_image_token,\n",
    ")\n",
    "from decord import cpu, VideoReader\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    \"./checkpoints/LongVU_Llama3_2_1B\", None, \"cambrian_llama\",load_8bit=True\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "video_path = \"./examples/video1.mp4\"\n",
    "qs = \"What's the prompt to generate the video?\"\n",
    "\n",
    "vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n",
    "fps = float(vr.get_avg_fps())\n",
    "frame_indices = np.array([i for i in range(0, len(vr), round(fps),)])\n",
    "video = []\n",
    "for frame_index in frame_indices:\n",
    "    img = vr[frame_index].asnumpy()\n",
    "    video.append(img)\n",
    "video = np.stack(video)\n",
    "image_sizes = [video[0].shape[:2]]\n",
    "video = process_images(video, image_processor, model.config)\n",
    "video = [item.unsqueeze(0) for item in video]\n",
    "\n",
    "qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\n",
    "conv = conv_templates[\"llama3\"].copy()\n",
    "conv.append_message(conv.roles[0], qs)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "\n",
    "input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(model.device)\n",
    "stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "keywords = [stop_str]\n",
    "stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n",
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        images=video,\n",
    "        image_sizes=image_sizes,\n",
    "        do_sample=False,\n",
    "        temperature=0.2,\n",
    "        max_new_tokens=128,\n",
    "        use_cache=True,\n",
    "        stopping_criteria=[stopping_criteria],\n",
    "    )\n",
    "pred = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "print(f\"\\nAnswer: {pred}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e266c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from longvu.builder import load_pretrained_model\n",
    "from longvu.constants import (\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    IMAGE_TOKEN_INDEX,\n",
    ")\n",
    "from longvu.conversation import conv_templates, SeparatorStyle\n",
    "from longvu.mm_datautils import (\n",
    "    KeywordsStoppingCriteria,\n",
    "    process_images,\n",
    "    tokenizer_image_token,\n",
    ")\n",
    "from decord import cpu, VideoReader\n",
    "\n",
    "model_path = \"new_checkpoints/cambrian_llama3_2_1B_25epoches/checkpoint-6000\"\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path, None, \"cambrian_llama\"\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "video_path = \"/home/fenghe/vidprom/cog_videos_example/cog-de3c22e1-6a93-5971-b09b-fc976ddd7803.mp4\"\n",
    "qs = \"What's the prompt to generate the video?\"\n",
    "\n",
    "vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n",
    "fps = float(vr.get_avg_fps())\n",
    "frame_indices = np.array([i for i in range(0, len(vr), round(fps),)])\n",
    "video = []\n",
    "for frame_index in frame_indices:\n",
    "    img = vr[frame_index].asnumpy()\n",
    "    video.append(img)\n",
    "video = np.stack(video)\n",
    "image_sizes = [video[0].shape[:2]]\n",
    "video = process_images(video, image_processor, model.config)\n",
    "video = [item.unsqueeze(0) for item in video]\n",
    "\n",
    "qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\n",
    "conv = conv_templates[\"llama3\"].copy()\n",
    "conv.append_message(conv.roles[0], qs)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "\n",
    "input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(model.device)\n",
    "stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "keywords = [stop_str]\n",
    "stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n",
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        images=video,\n",
    "        image_sizes=image_sizes,\n",
    "        do_sample=False,\n",
    "        temperature=0.2,\n",
    "        max_new_tokens=128,\n",
    "        use_cache=True,\n",
    "        stopping_criteria=[stopping_criteria],\n",
    "    )\n",
    "pred = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "print(f\"Model: {model_path}\\nAnswer: {pred}\") \n",
    "\n",
    "with open('video_inversion_answer.txt', 'a') as f:\n",
    "    f.write(f\"Model: {model_path}\\nAnswer: {pred}\\n\\n\")\n",
    "\n",
    "if 'model' in globals():\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf152c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from longvu.builder import load_pretrained_model\n",
    "from longvu.constants import (\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    IMAGE_TOKEN_INDEX,\n",
    ")\n",
    "from longvu.conversation import conv_templates, SeparatorStyle\n",
    "from longvu.mm_datautils import (\n",
    "    KeywordsStoppingCriteria,\n",
    "    process_images,\n",
    "    tokenizer_image_token,\n",
    ")\n",
    "from decord import cpu, VideoReader\n",
    "\n",
    "model_path = \"new_checkpoints/cambrian_llama3_2_1B_35epoches/checkpoint-35000\"\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path, None, \"cambrian_llama\"\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "video_path = \"/home/fenghe/vidprom/cog_videos_example/cog-0010fb2b-f6cd-51af-a0b6-b38812f965ea.mp4\"\n",
    "qs = \"What's the prompt to generate the video?\"\n",
    "\n",
    "vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n",
    "fps = float(vr.get_avg_fps())\n",
    "frame_indices = np.array([i for i in range(0, len(vr), round(fps),)])\n",
    "video = []\n",
    "for frame_index in frame_indices:\n",
    "    img = vr[frame_index].asnumpy()\n",
    "    video.append(img)\n",
    "video = np.stack(video)\n",
    "image_sizes = [video[0].shape[:2]]\n",
    "video = process_images(video, image_processor, model.config)\n",
    "video = [item.unsqueeze(0) for item in video]\n",
    "\n",
    "qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\n",
    "conv = conv_templates[\"llama3\"].copy()\n",
    "conv.append_message(conv.roles[0], qs)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "\n",
    "input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(model.device)\n",
    "stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "keywords = [stop_str]\n",
    "stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n",
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        images=video,\n",
    "        image_sizes=image_sizes,\n",
    "        do_sample=False,\n",
    "        temperature=0.2,\n",
    "        max_new_tokens=128,\n",
    "        use_cache=True,\n",
    "        stopping_criteria=[stopping_criteria],\n",
    "    )\n",
    "pred = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "print(f\"Model: {model_path}\\nAnswer: {pred}\") \n",
    "\n",
    "with open('video_inversion_answer.txt', 'a') as f:\n",
    "    f.write(f\"Model: {model_path}\\nAnswer: {pred}\\n\\n\")\n",
    "\n",
    "if 'model' in globals():\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "613ceb3b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Model: /home/fenghe/LongVU/new_checkpoints/cambrian_llama3_2_1B\n",
    "Answer: a man playing the saxophone in a garden\n",
    "\n",
    "Model: new_checkpoints/cambrian_llama3_2_1B_3epoches/checkpoint-3750\n",
    "Answer: a man playing saxophone in a garden\n",
    "\n",
    "Model: new_checkpoints/cambrian_llama3_2_1B_2epoches/checkpoint-2500\n",
    "Answer: a man playing saxophone in a garden"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dbb2b5f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ec67ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# 读取video_prompts.json\n",
    "with open('video_prompts.json', 'r') as f:\n",
    "    prompts = json.load(f)\n",
    "\n",
    "# 将prompts转换为DataFrame\n",
    "df = pd.DataFrame(prompts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c332e2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和测试集\n",
    "train_df = df.sample(frac=0.9, random_state=42)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "# 保存训练集和测试集，保留原来json的格式\n",
    "with open('train_prompts_90percent.json', 'w') as f:\n",
    "    json.dump(train_df.to_dict(orient='records'), f, indent=2)\n",
    "\n",
    "with open('test_prompts_10percent.json', 'w') as f:\n",
    "    json.dump(test_df.to_dict(orient='records'), f, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbc78cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import CogVideoXPipeline\n",
    "from diffusers.utils import export_to_video\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "pipe = CogVideoXPipeline.from_pretrained(\n",
    "    \"THUDM/CogVideoX-2b\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# pipe.enable_model_cpu_offload()\n",
    "# pipe.enable_sequential_cpu_offload()\n",
    "pipe.to(\"cuda\") \n",
    "pipe.vae.enable_slicing()\n",
    "pipe.vae.enable_tiling()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947a38d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = \"a purple car driving on the highway\"\n",
    "video = pipe(\n",
    "    prompt=prompt,\n",
    "    num_videos_per_prompt=1,\n",
    "    num_inference_steps=50,\n",
    "    num_frames=49,\n",
    "    guidance_scale=6,\n",
    "    generator=torch.Generator(device=\"cuda\").manual_seed(66),\n",
    ").frames[0]\n",
    "\n",
    "export_to_video(video, \"output/specific_examples/output_8.mp4\", fps=8)\n",
    "\n",
    "if 'pipe' in globals():\n",
    "    del video\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324174b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get started, PytorchAO needs to be installed from the GitHub source and PyTorch Nightly.\n",
    "# Source and nightly installation is only required until next release.\n",
    "\n",
    "import torch\n",
    "from diffusers import AutoencoderKLCogVideoX, CogVideoXTransformer3DModel, CogVideoXPipeline\n",
    "from diffusers.utils import export_to_video\n",
    "# from transformers import T5EncoderModel\n",
    "# from torchao.quantization import quantize_, int8_weight_only, int8_dynamic_activation_int8_weight\n",
    "\n",
    "# quantization = int8_weight_only\n",
    "\n",
    "# text_encoder = T5EncoderModel.from_pretrained(\"THUDM/CogVideoX-5b\", subfolder=\"text_encoder\", torch_dtype=torch.bfloat16)\n",
    "# quantize_(text_encoder, quantization())\n",
    "\n",
    "# transformer = CogVideoXTransformer3DModel.from_pretrained(\"THUDM/CogVideoX-5b\", subfolder=\"transformer\", torch_dtype=torch.bfloat16)\n",
    "# quantize_(transformer, quantization())\n",
    "\n",
    "# vae = AutoencoderKLCogVideoX.from_pretrained(\"THUDM/CogVideoX-2b\", subfolder=\"vae\", torch_dtype=torch.bfloat16)\n",
    "# quantize_(vae, quantization())\n",
    "\n",
    "# Create pipeline and run inference\n",
    "pipe = CogVideoXPipeline.from_pretrained(\n",
    "    \"THUDM/CogVideoX-2b\",\n",
    "    # text_encoder=text_encoder,\n",
    "    # transformer=transformer,\n",
    "    # vae=vae,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "pipe.to(\"cuda\")\n",
    "pipe.enable_model_cpu_offload()\n",
    "pipe.vae.enable_tiling()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fbcced",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = \"A woman is smoking a cigarette while sitting on a bed.\"\n",
    "\n",
    "video = pipe(\n",
    "    prompt=prompt,\n",
    "    num_videos_per_prompt=1,\n",
    "    num_inference_steps=50,\n",
    "    num_frames=49,\n",
    "    guidance_scale=6,\n",
    "    generator=torch.Generator(device=\"cuda\").manual_seed(42),\n",
    ").frames[0]\n",
    "\n",
    "export_to_video(video, \"output.mp4\", fps=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c2e38a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "longvu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
