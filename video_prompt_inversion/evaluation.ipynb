{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import argparse\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    \"\"\"Load JSON file and return as Python object\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def extract_video_name(path):\n",
    "    \"\"\"Extract video filename without extension from path\"\"\"\n",
    "    return os.path.splitext(os.path.basename(path))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_path = 'output/batch_video_results.json'\n",
    "references_path = 'datasets/video_prompts.json'\n",
    "#output_path = 'output/batch_video_results.csv'\n",
    "\n",
    "\n",
    "# Load JSON files\n",
    "predictions_data = load_json_file(prediction_path)\n",
    "references_data = load_json_file(references_path)\n",
    "\n",
    "if not predictions_data or not references_data:\n",
    "    print(\"Failed to load one or both JSON files. Exiting.\")\n",
    "\n",
    "# Create a dictionary of reference prompts keyed by video name\n",
    "reference_dict = {}\n",
    "for item in references_data:\n",
    "    # Adapt this according to your reference JSON structure\n",
    "    if isinstance(item, dict) and 'video' in item and 'conversations' in item:\n",
    "        reference_dict[item['video']] = item['conversations'][1]['value']\n",
    "\n",
    "\n",
    "# Create a list to store all comparison data\n",
    "comparison_data = []\n",
    "\n",
    "# Process each prediction\n",
    "for pred in predictions_data:\n",
    "    video_name = pred['video_name']\n",
    "    prediction = pred.get('prediction', '')\n",
    "    \n",
    "    # Find matching reference\n",
    "    reference = reference_dict.get(video_name, '')\n",
    "    \n",
    "    \n",
    "    # Add to comparison data\n",
    "    comparison_data.append({\n",
    "        'video_name': video_name,\n",
    "        'reference': reference,\n",
    "        'prediction': prediction,\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(comparison_data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save to CSV\n",
    "df.to_parquet('output/compare_evalutaion_results.parquet', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result_df = pd.read_parquet('output/compare_evalutaion_results.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bert_score import score\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def Calmetic(references:list[list[str]], predictions:list[str]):\n",
    "    '''\n",
    "    Input format:\n",
    "\n",
    "    predictions = [\n",
    "        \"What is the capital of France?\",\n",
    "        \"Who wrote the book?\",\n",
    "        \"What is the largest planet?\"\n",
    "    ]\n",
    "\n",
    "    references = [\n",
    "        [\"What is the capital city of France?\"],\n",
    "        [\"Who is the author of the book?\"],\n",
    "        [\"Which planet is the largest in the solar system?\"]\n",
    "    ]\n",
    "    '''\n",
    "\n",
    "    # # 加载 BLEU 评分器\n",
    "    # bleu_metric = load_metric(\"bleu\")\n",
    "\n",
    "    # # 计算 BLEU 分数\n",
    "    predictions_tokenized = [word_tokenize(pred) for pred in predictions]\n",
    "    references_tokenized = [[word_tokenize(refs[0])] for refs in references]\n",
    "    # B_S = {}\n",
    "    # for n in range(1, 5):\n",
    "    #     bleu_metric.add_batch(predictions=predictions_tokenized, references=references_tokenized)\n",
    "    #     results = bleu_metric.compute(max_order=n)\n",
    "    #     B_S[f\"BLEU-{n}\"] = results\n",
    "    bleu_metric = evaluate.load(\"bleu\")\n",
    "    B_S = bleu_metric.compute(predictions=predictions, references=references,tokenizer=word_tokenize)\n",
    "    for i,n in enumerate(B_S['precisions']):\n",
    "        print(f\"BLEU-{i+1} score: {n:.5f}\")\n",
    "        \n",
    "\n",
    "\n",
    "    # 加载 ROUGE 评分器\n",
    "    rouge_metric = load_metric(\"rouge\")\n",
    "    '''\n",
    "    ROUGE-1: 衡量生成文本和参考文本之间的 unigram 匹配。\n",
    "    ROUGE-2: 衡量生成文本和参考文本之间的 bigram 匹配。\n",
    "    ROUGE-L: 衡量生成文本和参考文本之间的最长公共子序列(LCS)。\n",
    "    ROUGE-Lsum: 基于 LCS 的一个变体，专门用于长文本的评估。\n",
    "    '''\n",
    "    # 计算 ROUGE 分数\n",
    "    rouge_results = rouge_metric.compute(predictions=predictions, references=references)\n",
    "    rouge1_mid_f1 = rouge_results['rouge1'][1][2]\n",
    "    rouge2_mid_f1 = rouge_results['rouge2'][1][2]\n",
    "    rougeL_mid_f1 = rouge_results['rougeL'][1][2]\n",
    "    rougeLsum_mid_f1 = rouge_results['rougeLsum'][1][2]\n",
    "    print(f\"ROUGE-1 F1 score: {rouge1_mid_f1:.5f}\")\n",
    "    print(f\"ROUGE-2 F1 score: {rouge2_mid_f1:.5f}\")\n",
    "    print(f\"ROUGE-L F1 score: {rougeL_mid_f1:.5f}\")\n",
    "    print(f\"ROUGE-Lsum F1 score: {rougeLsum_mid_f1:.5f}\")\n",
    "\n",
    "    # 计算 METEOR 分数\n",
    "    meteor_scores = [meteor_score(references=refs, hypothesis=pred) for pred, refs in zip(predictions_tokenized, references_tokenized)]\n",
    "    average_meteor_score = sum(meteor_scores) / len(meteor_scores)\n",
    "    print(f\"Average METEOR score: {average_meteor_score:.5f}\")\n",
    "\n",
    "    # 计算 BERTScore 分数\n",
    "\n",
    "    P, R, F1 = score(predictions, [ref[0] for ref in references], lang=\"en\", verbose=False)\n",
    "    average_bert_score = F1.mean().item()\n",
    "    print(f\"Average BERTScore F1: {average_bert_score:.5f}\")\n",
    "\n",
    "    return {\n",
    "        \"BLEU\":B_S,\n",
    "        \"ROUGE\":rouge_results,\n",
    "        \"METERO\":meteor_scores,\n",
    "        \"BERTScore\":{\"Precision\":P,\"Recall\":R,\"F1\":F1},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = [ [i] for i in result_df['reference']]\n",
    "content = result_df['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_sentence(s):\n",
    "    s = s.replace(\"，\", \",\")  # 替换中文逗号\n",
    "    s = re.sub(r\"-[a-z]+\\s*\\d+\", \"\", s)  # 去掉如 -fps 24 这种参数\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "predictions = [clean_sentence(p) for p in result_df['prediction']]\n",
    "references = [[clean_sentence(r)] for r in result_df['reference']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = Calmetic(references=references,predictions=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两个重要数值\n",
    "print(res['BLEU'])\n",
    "print(res['ROUGE']['rougeLsum'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity as torch_cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')  #SentenceTransformer(\"bert-base-uncased\") \n",
    "\n",
    "reference_texts_ = [ clean_sentence(i) for i in result_df['reference'] ]\n",
    "embeddings1 = sentence_model.encode(predictions, convert_to_tensor=True)\n",
    "embeddings2 = sentence_model.encode(result_df['reference'], convert_to_tensor=True)\n",
    "\n",
    "cosine_scores_2 = util.pytorch_cos_sim(embeddings1, embeddings2)  \n",
    "\n",
    "# 输出余弦相似度的值\n",
    "print(f\"Average Cosine Similarity: {cosine_scores_2.diagonal().mean()}\")\n",
    "print(f\"Biggest Cosine Similarity: {cosine_scores_2.diagonal().max()}\")\n",
    "print(f\"Middle Cosine Similarity: {cosine_scores_2.diagonal().median()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 获取余弦相似度数据\n",
    "cos_sim_scores = cosine_scores_2.diagonal()\n",
    "\n",
    "# 创建直方图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(cos_sim_scores.cpu().numpy(), bins=50, edgecolor='black')\n",
    "plt.title('Distribution of Cosine Similarity Scores')\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['cosine_similarity'] = cos_sim_scores.cpu().numpy().tolist()\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_parquet('output/result_df_cosine_similarity.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result_df = pd.read_parquet('output/result_df_cosine_similarity.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在result_df中找测试集datasets/test_prompts_20percent.json中对应video_name的行，形成一个新的dataframe\n",
    "# Read test prompts json file\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open('datasets/test_prompts_20percent.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Get list of video names from test set\n",
    "test_video_names = [item['video'] for item in test_data]\n",
    "\n",
    "# Filter result_df to only include videos from test set\n",
    "test_result_df = result_df[result_df['video_name'].isin(test_video_names)]\n",
    "\n",
    "# Print statistics about cosine similarity scores for test set\n",
    "test_cos_scores = test_result_df['cosine_similarity']\n",
    "print(f\"Test Set Statistics:\")\n",
    "print(f\"Average Cosine Similarity: {test_cos_scores.mean():.4f}\")\n",
    "print(f\"Max Cosine Similarity: {test_cos_scores.max():.4f}\") \n",
    "print(f\"Median Cosine Similarity: {test_cos_scores.median():.4f}\")\n",
    "\n",
    "# Plot distribution of cosine similarities for test set\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(test_cos_scores, bins=50, edgecolor='black')\n",
    "plt.title('Distribution of Cosine Similarity Scores (Test Set)')\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_df.to_parquet('output/test_result_df_cosine_similarity.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_df =pd.read_parquet('output/test_result_df_cosine_similarity.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cos_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices where cosine similarity > 0.5\n",
    "high_test_indices = [i for i, score in enumerate(test_cos_scores) if score > 0.7]\n",
    "\n",
    "# Create a new dataframe with high similarity pairs\n",
    "high_test_sim_df = pd.DataFrame({\n",
    "    'video_name': [test_result_df['video_name'][i] for i in high_test_indices],\n",
    "    'reference': [test_result_df['reference'][i] for i in high_test_indices],\n",
    "    'prediction': [test_result_df['prediction'][i] for i in high_test_indices],\n",
    "    'similarity_score': [test_result_df['cosine_similarity'][i] for i in high_test_indices]\n",
    "})\n",
    "\n",
    "# Sort by similarity score in descending order\n",
    "#high_test_sim_df = high_test_sim_df.sort_values('similarity_score', ascending=False)\n",
    "\n",
    "print(f\"Number of pairs with similarity > 0.7: {len(high_test_sim_df)}\")\n",
    "display(high_test_sim_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_sentence(s):\n",
    "    s = s.replace(\"，\", \",\")  # 替换中文逗号\n",
    "    s = re.sub(r\"-[a-z]+\\s*\\d+\", \"\", s)  # 去掉如 -fps 24 这种参数\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "predictions_test = [p for p in high_test_sim_df['prediction']]\n",
    "references_test = [[r] for r in high_test_sim_df['reference']]\n",
    "\n",
    "res_test = Calmetic(references=references_test,predictions=predictions_test)\n",
    "print(res_test)\n",
    "print(res_test['BLEU'])\n",
    "print(res_test['ROUGE']['rougeLsum'][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity as torch_cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')  #SentenceTransformer(\"bert-base-uncased\") \n",
    "\n",
    "embeddings1 = sentence_model.encode(high_test_sim_df['prediction'], convert_to_tensor=True)\n",
    "embeddings2 = sentence_model.encode(high_test_sim_df['reference'], convert_to_tensor=True)\n",
    "\n",
    "cosine_scores_2 = util.pytorch_cos_sim(embeddings1, embeddings2)  \n",
    "\n",
    "# 输出余弦相似度的值\n",
    "print(f\"Average Cosine Similarity: {cosine_scores_2.diagonal().mean()}\")\n",
    "print(f\"Biggest Cosine Similarity: {cosine_scores_2.diagonal().max()}\")\n",
    "print(f\"Middle Cosine Similarity: {cosine_scores_2.diagonal().median()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_test_sim_df.to_parquet('output/high_test_over0.5_sim_df.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices where cosine similarity > 0.7\n",
    "high_sim_indices = (cos_sim_scores > 0.6).nonzero().squeeze().tolist()\n",
    "\n",
    "# Create a new dataframe with high similarity pairs\n",
    "high_sim_df = pd.DataFrame({\n",
    "    'video_name': [result_df['video_name'][i] for i in high_sim_indices],\n",
    "    'reference': [result_df['reference'][i] for i in high_sim_indices],\n",
    "    'prediction': [result_df['prediction'][i] for i in high_sim_indices],\n",
    "    'similarity_score': cos_sim_scores[high_sim_indices].cpu().numpy()\n",
    "})\n",
    "\n",
    "# Sort by similarity score in descending order\n",
    "high_sim_df = high_sim_df.sort_values('similarity_score', ascending=False)\n",
    "\n",
    "print(f\"Number of pairs with similarity > 0.7: {len(high_sim_df)}\")\n",
    "display(high_sim_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_sim_df.to_parquet('output/high_sim_over0.6_df.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_sentence(s):\n",
    "    s = s.replace(\"，\", \",\")  # 替换中文逗号\n",
    "    s = re.sub(r\"-[a-z]+\\s*\\d+\", \"\", s)  # 去掉如 -fps 24 这种参数\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "predictions = [p for p in high_sim_df['prediction']]\n",
    "references = [[r] for r in high_sim_df['reference']]\n",
    "\n",
    "res = Calmetic(references=references,predictions=predictions)\n",
    "print(res['BLEU'])\n",
    "print(res['ROUGE']['rougeLsum'][1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity as torch_cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')  #SentenceTransformer(\"bert-base-uncased\") \n",
    "\n",
    "embeddings1 = sentence_model.encode(high_sim_df['prediction'], convert_to_tensor=True)\n",
    "embeddings2 = sentence_model.encode(high_sim_df['reference'], convert_to_tensor=True)\n",
    "\n",
    "cosine_scores_2 = util.pytorch_cos_sim(embeddings1, embeddings2)  \n",
    "\n",
    "# 输出余弦相似度的值\n",
    "print(f\"Average Cosine Similarity: {cosine_scores_2.diagonal().mean()}\")\n",
    "print(f\"Biggest Cosine Similarity: {cosine_scores_2.diagonal().max()}\")\n",
    "print(f\"Middle Cosine Similarity: {cosine_scores_2.diagonal().median()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vec2txt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
