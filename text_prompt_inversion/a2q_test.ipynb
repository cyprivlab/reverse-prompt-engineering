{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset and re-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SQuAD dataset\n",
    "dataset = load_dataset('squad')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Different models training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -t5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 87599/87599 [00:03<00:00, 27009.18 examples/s]\n",
      "Map: 100%|██████████| 10570/10570 [00:00<00:00, 31216.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_t5_data(example):\n",
    "    if example['answers']['text']:\n",
    "        answer_text = example['answers']['text'][0]\n",
    "    else:\n",
    "        answer_text = \"No answer found\"\n",
    "    return {\n",
    "        'input_text': f\"answer: {answer_text}\",\n",
    "        'target_text': f\"question: {example['question']}\" \n",
    "    }\n",
    "\n",
    "processed_t5_dataset = dataset.map(preprocess_t5_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split_t5 = processed_t5_dataset['train'].train_test_split(test_size=0.1)\n",
    "train_dataset_t5 = train_test_split_t5['train']\n",
    "val_dataset_t5 = train_test_split_t5['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "def tokenize_t5_function(examples):\n",
    "    model_inputs = t5_tokenizer(examples['input_text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    labels = t5_tokenizer(examples['target_text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train_dataset_t5 = train_dataset_t5.map(tokenize_t5_function, batched=True)\n",
    "tokenized_val_dataset_t5 = val_dataset_t5.map(tokenize_t5_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args_t5 = TrainingArguments(\n",
    "    output_dir='./results_t5_small',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer_t5 = Trainer(\n",
    "    model=t5_model,\n",
    "    args=training_args_t5,\n",
    "    train_dataset=tokenized_train_dataset_t5,\n",
    "    eval_dataset=tokenized_val_dataset_t5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer_t5.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question(answer):\n",
    "    t5_model.eval()  \n",
    "    input_ids = t5_tokenizer.encode(\"answer: \" + answer, return_tensors=\"pt\").to(device)\n",
    "    outputs = t5_model.generate(input_ids, max_length=64, num_beams=5, early_stopping=True)\n",
    "    question = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Habitat Conservation Plan\n",
      "Generated Question: question: What is the name of the plan that aims to protect wildlife from extinction?\n",
      "Actual Question: question: What program gives incentives to private landowners to protect species on their land?\n",
      "\n",
      "Answer: Vedantins\n",
      "Generated Question: question: What is the name of the sulfate that can be used to produce uranium?\n",
      "Actual Question: question: What school thought that language was supposed to be widened to describe and develop?\n",
      "\n",
      "Answer: the Bagratid Dynasty\n",
      "Generated Question: question: What was the name of the dynasty that led to the end of the Ottoman empire?\n",
      "Actual Question: question: What dynasty was Ashot I part of?\n",
      "\n",
      "Answer: RES Directive\n",
      "Generated Question: question: What is the name of the directive that governs the use of uranium uranium?\n",
      "Actual Question: question: What states that EU Member States must ensure that the origin of electricity produced from renewables can be guaranteed?\n",
      "\n",
      "Answer: modern-day Eritrea\n",
      "Generated Question: question: What is the name of the region that has the largest population in the world?\n",
      "Actual Question: question: What is the result of the incorporation of independent kingdoms and sultanates?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "samples = val_dataset_t5.shuffle(seed=42).select(range(5)) \n",
    "\n",
    "\n",
    "for example in samples:\n",
    "    generated_question = generate_question(example['input_text'].replace(\"answer: \", \"\"))\n",
    "    print(f\"Answer: {example['input_text'].replace('answer: ', '')}\")\n",
    "    print(f\"Generated Question: {generated_question}\")\n",
    "    print(f\"Actual Question: {example['target_text']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -bart-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 87599/87599 [00:02<00:00, 29545.22 examples/s]\n",
      "Map: 100%|██████████| 10570/10570 [00:00<00:00, 30497.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocess_squad(example):\n",
    "    # Invert the dataset by treating the answer as input and the question as output\n",
    "    if example['answers']['text']:\n",
    "    # SQuAD has answers as a list of possible answer texts; we'll just use the first one for simplicity\n",
    "        answer_text = example['answers']['text'][0]\n",
    "    else:\n",
    "        answer_text = \"No answer found\"\n",
    "    return {\n",
    "        'input_text': f\"answer: {answer_text}\",\n",
    "        'target_text': example['question']\n",
    "    }\n",
    "\n",
    "\n",
    "# Preprocess the dataset\n",
    "processed_dataset = dataset.map(preprocess_squad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = processed_dataset['train'].train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split['train']\n",
    "val_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78839, 7)\n",
      "(8760, 7)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.shape)\n",
    "print(val_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 78839/78839 [00:14<00:00, 5359.62 examples/s]\n",
      "Map: 100%|██████████| 8760/8760 [00:02<00:00, 4341.60 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer\n",
    "\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "\n",
    "def tokenize_bart_function(examples):\n",
    "    model_inputs = bart_tokenizer(examples['input_text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    labels = bart_tokenizer(examples['target_text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train_dataset_bart = train_dataset.map(tokenize_bart_function, batched=True)\n",
    "tokenized_val_dataset_bart = val_dataset.map(tokenize_bart_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large').to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args_bart = TrainingArguments(\n",
    "    output_dir='./results_bart_2',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer_bart = Trainer(\n",
    "    model=bart_model,\n",
    "    args=training_args_bart,\n",
    "    train_dataset=tokenized_train_dataset_bart,\n",
    "    eval_dataset=tokenized_val_dataset_bart\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_bart.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question(answer):\n",
    "    bart_model.eval()  \n",
    "    input_ids = bart_tokenizer.encode(\"answer: \" + answer, return_tensors=\"pt\").to(device)\n",
    "    outputs = bart_model.generate(input_ids, max_length=64, num_beams=5, early_stopping=True)\n",
    "    question = bart_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: power was transferred to the eldest member\n",
      "Generated Question: What happened to the power of the members of the upper house after the election?\n",
      "Actual Question: What did the rota system do?\n",
      "\n",
      "Answer: the Bipartisan Campaign Reform Act of 2002\n",
      "Generated Question: What act was passed in 2002 to reform campaign spending?\n",
      "Actual Question: What finance act affected the 2004 election?\n",
      "\n",
      "Answer: directly attached to the end of the motor\n",
      "Generated Question: Where is the rotor located in a synchronous motor?\n",
      "Actual Question: In a gearless traction engine, what is the drive sheave attached to?\n",
      "\n",
      "Answer: Neolithic cave-burial\n",
      "Generated Question: What is the earliest evidence of human activity in Myanmar?\n",
      "Actual Question: What kind of bural was at Adaoutse, Bouches-du-Rhône?\n",
      "\n",
      "Answer: 960–1279\n",
      "Generated Question: When did the Tang dynasty rule?\n",
      "Actual Question: When did the Song dynasty take place?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "samples = val_dataset.shuffle(seed=42).select(range(5))  \n",
    "\n",
    "\n",
    "for example in samples:\n",
    "    generated_question = generate_question(example['input_text'].replace(\"answer: \", \"\"))\n",
    "    print(f\"Answer: {example['input_text'].replace('answer: ', '')}\")\n",
    "    print(f\"Generated Question: {generated_question}\")\n",
    "    print(f\"Actual Question: {example['target_text']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -t5-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_t5_data(example):\n",
    "    if example['answers']['text']:\n",
    "        answer_text = example['answers']['text'][0]\n",
    "    else:\n",
    "        answer_text = \"No answer found\"\n",
    "    return {\n",
    "        'input_text': f\"answer: {answer_text}\",\n",
    "        'target_text': f\"question: {example['question']}\"\n",
    "    }\n",
    "\n",
    "processed_t5_dataset = dataset.map(preprocess_t5_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split_t5 = processed_t5_dataset['train'].train_test_split(test_size=0.1)\n",
    "train_dataset_t5 = train_test_split_t5['train']\n",
    "val_dataset_t5 = train_test_split_t5['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "def tokenize_t5_function(examples):\n",
    "    model_inputs = t5_tokenizer(examples['input_text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    labels = t5_tokenizer(examples['target_text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train_dataset_t5 = train_dataset_t5.map(tokenize_t5_function, batched=True)\n",
    "tokenized_val_dataset_t5 = val_dataset_t5.map(tokenize_t5_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "t5_base_model = T5ForConditionalGeneration.from_pretrained('t5-base').to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args_t5 = TrainingArguments(\n",
    "    output_dir='./results_t5_base',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer_t5 = Trainer(\n",
    "    model=t5_base_model,\n",
    "    args=training_args_t5,\n",
    "    train_dataset=tokenized_train_dataset_t5,\n",
    "    eval_dataset=tokenized_val_dataset_t5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer_t5.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question(answer):\n",
    "    t5_base_model.eval()  \n",
    "    input_ids = t5_tokenizer.encode(\"answer: \" + answer, return_tensors=\"pt\").to(device)\n",
    "    outputs = t5_base_model.generate(input_ids, max_length=64, num_beams=5, early_stopping=True)\n",
    "    question = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: power was transferred to the eldest member\n",
      "Generated Question: question: What happens when a member of a federation loses power?\n",
      "Actual Question: What did the rota system do?\n",
      "\n",
      "Answer: the Bipartisan Campaign Reform Act of 2002\n",
      "Generated Question: question: What was the name of the bill that was passed by Congress in 2002?\n",
      "Actual Question: What finance act affected the 2004 election?\n",
      "\n",
      "Answer: directly attached to the end of the motor\n",
      "Generated Question: question: How is the servo motor mounted?\n",
      "Actual Question: In a gearless traction engine, what is the drive sheave attached to?\n",
      "\n",
      "Answer: Neolithic cave-burial\n",
      "Generated Question: question: What type of burial was held at the site?\n",
      "Actual Question: What kind of bural was at Adaoutse, Bouches-du-Rhône?\n",
      "\n",
      "Answer: 960–1279\n",
      "Generated Question: question: What was the span of the Han dynasty?\n",
      "Actual Question: When did the Song dynasty take place?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "samples = val_dataset.shuffle(seed=42).select(range(5))  \n",
    "\n",
    "\n",
    "for example in samples:\n",
    "    generated_question = generate_question(example['input_text'].replace(\"answer: \", \"\"))\n",
    "    print(f\"Answer: {example['input_text'].replace('answer: ', '')}\")\n",
    "    print(f\"Generated Question: {generated_question}\")\n",
    "    print(f\"Actual Question: {example['target_text']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL4LM3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
