{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset and re-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset('vicgalle/alpaca-gpt4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Different models training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DI_model_save_name = 'compare_work/DI_GPTRQA_t5_small_5e'\n",
    "DI_generation_texts_pth = 'compare_work/DI_GTPRQA_gen_5e.txt'\n",
    "DI_FT_generation_texts_pth = 'compare_work/DI_FT_GPTRQA_gen_5e.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DI-t5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_t5_data(example,index):\n",
    "    if example['output']:\n",
    "        answer_text = example['output']\n",
    "    else:\n",
    "        answer_text = \"No answer found\"\n",
    "    return {\n",
    "        'index':index,\n",
    "        'input_text': f\"answer: {answer_text}\",\n",
    "        'target_text': f\"enquiry: {example['instruction']+' '+example['input']}\"  \n",
    "    }\n",
    "\n",
    "processed_t5small_dataset = dataset.map(preprocess_t5_data,with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_t5small_dataset['train']['target_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split_t5 = processed_t5small_dataset['train'].train_test_split(test_size=0.2)\n",
    "train_dataset_t5 = train_test_split_t5['train']\n",
    "val_dataset_t5 = train_test_split_t5['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_t5.save_to_disk(dataset_path='compare_work/alpaca-0.2-train')\n",
    "val_dataset_t5.save_to_disk(dataset_path='compare_work/alpaca-0.2-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,load_from_disk\n",
    "\n",
    "train_dataset_t5 = load_from_disk('Ans2Seq/compare_work/GPTRQA-train')\n",
    "val_dataset_t5 = load_from_disk('Ans2Seq/compare_work/GPTRQA-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "def tokenize_t5_function(examples):\n",
    "    model_inputs = t5_tokenizer(examples['input_text'], padding=\"max_length\", truncation=True)\n",
    "    labels = t5_tokenizer(examples['target_text'], padding=\"max_length\", truncation=True)\n",
    "    print(labels)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train_dataset_t5 = train_dataset_t5.map(tokenize_t5_function, batched=True)\n",
    "tokenized_val_dataset_t5 = val_dataset_t5.map(tokenize_t5_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args_t5 = TrainingArguments(\n",
    "    output_dir= DI_model_save_name,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=5,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer_t5 = Trainer(\n",
    "    model=t5_model,\n",
    "    args=training_args_t5,\n",
    "    train_dataset=tokenized_train_dataset_t5,\n",
    "    eval_dataset=tokenized_val_dataset_t5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_t5.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_t5.save_model(DI_model_save_name) \n",
    "t5_tokenizer.save_pretrained(DI_model_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question(answer):\n",
    "    t5_model.eval()  \n",
    "    input_ids = t5_tokenizer.encode(\"answer: \" + answer, return_tensors=\"pt\").to(device)\n",
    "    outputs = t5_model.generate(input_ids, num_beams=5, early_stopping=True)\n",
    "    question = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "samples = val_dataset_t5  \n",
    "res = []\n",
    "\n",
    "with open(DI_generation_texts_pth, 'a') as file:\n",
    "    for example in tqdm(samples):\n",
    "        generated_question = generate_question(example['input_text'].replace(\"answer: \", \"\"))\n",
    "        res.append(generated_question.replace(\"enquiry: \", \"\"))\n",
    "        file.write((generated_question.replace(\"enquiry: \", \"\")+'\\n'))\n",
    "        # print(f\"Answer: {example['input_text'].replace('answer: ', '')}\")\n",
    "        # print(f\"Generated Question: {generated_question}\")\n",
    "        # print(f\"Actual Question: {example['target_text']}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bert_score import score\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def Calmetic(references:list[list[str]], predictions:list[str]):\n",
    "    '''\n",
    "    Input format:\n",
    "\n",
    "    predictions = [\n",
    "        \"What is the capital of France?\",\n",
    "        \"Who wrote the book?\",\n",
    "        \"What is the largest planet?\"\n",
    "    ]\n",
    "\n",
    "    references = [\n",
    "        [\"What is the capital city of France?\"],\n",
    "        [\"Who is the author of the book?\"],\n",
    "        [\"Which planet is the largest in the solar system?\"]\n",
    "    ]\n",
    "    '''\n",
    "\n",
    "    # # Load BLEU scorer\n",
    "    # bleu_metric = load_metric(\"bleu\")\n",
    "\n",
    "    # # Calculate BLEU score\n",
    "    predictions_tokenized = [word_tokenize(pred) for pred in predictions]\n",
    "    references_tokenized = [[word_tokenize(refs[0])] for refs in references]\n",
    "    # B_S = {}\n",
    "    # for n in range(1, 5):\n",
    "    #     bleu_metric.add_batch(predictions=predictions_tokenized, references=references_tokenized)\n",
    "    #     results = bleu_metric.compute(max_order=n)\n",
    "    #     B_S[f\"BLEU-{n}\"] = results\n",
    "    bleu_metric = evaluate.load(\"bleu\")\n",
    "    B_S = bleu_metric.compute(predictions=predictions, references=references,tokenizer=word_tokenize)\n",
    "    for i,n in enumerate(B_S['precisions']):\n",
    "        print(f\"BLEU-{i+1} score: {n:.5f}\")\n",
    "        \n",
    "\n",
    "\n",
    "    # Load ROUGE scorer\n",
    "    rouge_metric = load_metric(\"rouge\")\n",
    "    '''\n",
    "    ROUGE-1: Measures unigram matches between generated text and reference text.\n",
    "    ROUGE-2: Measures bigram matches between generated text and reference text.\n",
    "    ROUGE-L: Measures the longest common subsequence (LCS) between generated text and reference text.\n",
    "    ROUGE-Lsum: A variant based on LCS, specifically designed for evaluating long texts.\n",
    "    '''\n",
    "    # Calculate ROUGE scores\n",
    "    rouge_results = rouge_metric.compute(predictions=predictions, references=references)\n",
    "    rouge1_mid_f1 = rouge_results['rouge1'][1][2]\n",
    "    rouge2_mid_f1 = rouge_results['rouge2'][1][2]\n",
    "    rougeL_mid_f1 = rouge_results['rougeL'][1][2]\n",
    "    rougeLsum_mid_f1 = rouge_results['rougeLsum'][1][2]\n",
    "    print(f\"ROUGE-1 F1 score: {rouge1_mid_f1:.5f}\")\n",
    "    print(f\"ROUGE-2 F1 score: {rouge2_mid_f1:.5f}\")\n",
    "    print(f\"ROUGE-L F1 score: {rougeL_mid_f1:.5f}\")\n",
    "    print(f\"ROUGE-Lsum F1 score: {rougeLsum_mid_f1:.5f}\")\n",
    "\n",
    "    # Calculate METEOR score\n",
    "    meteor_scores = [meteor_score(references=refs, hypothesis=pred) for pred, refs in zip(predictions_tokenized, references_tokenized)]\n",
    "    average_meteor_score = sum(meteor_scores) / len(meteor_scores)\n",
    "    print(f\"Average METEOR score: {average_meteor_score:.5f}\")\n",
    "\n",
    "    # Calculate BERTScore\n",
    "\n",
    "    P, R, F1 = score(predictions, [ref[0] for ref in references], lang=\"en\", verbose=False)\n",
    "    average_bert_score = F1.mean().item()\n",
    "    print(f\"Average BERTScore F1: {average_bert_score:.5f}\")\n",
    "\n",
    "    return {\n",
    "        \"BLEU\":B_S,\n",
    "        \"ROUGE\":rouge_results,\n",
    "        \"METERO\":meteor_scores,\n",
    "        \"BERTScore\":{\"Precision\":P,\"Recall\":R,\"F1\":F1},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DI_generation_texts_pth, 'r') as file:\n",
    "    content = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = [ [i.replace('enquiry: ',\"\")] for i in val_dataset_t5['target_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = Calmetic(references=refs,predictions=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res['BLEU'])\n",
    "print(res['ROUGE']['rougeLsum'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_t5['target_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity as torch_cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')#SentenceTransformer(\"bert-base-uncased\") \n",
    "\n",
    "reference_texts_ = [ i.replace('enquiry: ',\"\") for i in val_dataset_t5['target_text'] ]\n",
    "embeddings1 = sentence_model.encode(content, convert_to_tensor=True)\n",
    "embeddings2 = sentence_model.encode(reference_texts_, convert_to_tensor=True)\n",
    "\n",
    "cosine_scores_2 = util.pytorch_cos_sim(embeddings1, embeddings2)   #[52002,52002]维度的矩阵，对角线上的值为对应文本的余弦相似度\n",
    "\n",
    "# 输出余弦相似度的值\n",
    "print(f\"Average Cosine Similarity: {cosine_scores_2.diagonal().mean()}\")\n",
    "print(f\"Biggest Cosine Similarity: {cosine_scores_2.diagonal().max()}\")\n",
    "print(f\"Middle Cosine Similarity: {cosine_scores_2.diagonal().median()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -bart-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_squad(example):\n",
    "    # Invert the dataset by treating the answer as input and the question as output\n",
    "    if example['answers']['text']:\n",
    "    # SQuAD has answers as a list of possible answer texts; we'll just use the first one for simplicity\n",
    "        answer_text = example['answers']['text'][0]\n",
    "    else:\n",
    "        answer_text = \"No answer found\"\n",
    "    return {\n",
    "        'input_text': f\"answer: {answer_text}\",\n",
    "        'target_text': example['question']\n",
    "    }\n",
    "\n",
    "\n",
    "# Preprocess the dataset\n",
    "processed_dataset = dataset.map(preprocess_squad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = processed_dataset['train'].train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split['train']\n",
    "val_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset.shape)\n",
    "print(val_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "\n",
    "def tokenize_bart_function(examples):\n",
    "    model_inputs = bart_tokenizer(examples['input_text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    labels = bart_tokenizer(examples['target_text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train_dataset_bart = train_dataset.map(tokenize_bart_function, batched=True)\n",
    "tokenized_val_dataset_bart = val_dataset.map(tokenize_bart_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large').to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args_bart = TrainingArguments(\n",
    "    output_dir='./results_bart_2',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer_bart = Trainer(\n",
    "    model=bart_model,\n",
    "    args=training_args_bart,\n",
    "    train_dataset=tokenized_train_dataset_bart,\n",
    "    eval_dataset=tokenized_val_dataset_bart\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_bart.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question(answer):\n",
    "    bart_model.eval()  \n",
    "    input_ids = bart_tokenizer.encode(\"answer: \" + answer, return_tensors=\"pt\").to(device)\n",
    "    outputs = bart_model.generate(input_ids, max_length=64, num_beams=5, early_stopping=True)\n",
    "    question = bart_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "samples = val_dataset.shuffle(seed=42).select(range(5))  \n",
    "\n",
    "\n",
    "for example in samples:\n",
    "    generated_question = generate_question(example['input_text'].replace(\"answer: \", \"\"))\n",
    "    print(f\"Answer: {example['input_text'].replace('answer: ', '')}\")\n",
    "    print(f\"Generated Question: {generated_question}\")\n",
    "    print(f\"Actual Question: {example['target_text']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DI+FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "#train_dataset_t5 = load_from_disk('Ans2Seq/compare_work/alpaca-0.2-train')\n",
    "val_dataset_t5 = load_from_disk('Ans2Seq/compare_work/GPTRQA-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(val_dataset_t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "def tokenize_t5_function(examples):\n",
    "    model_inputs = t5_tokenizer(examples['input_text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    labels = t5_tokenizer(examples['target_text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "#tokenized_train_dataset_t5 = train_dataset_t5.map(tokenize_t5_function, batched=True)\n",
    "tokenized_val_dataset_t5 = val_dataset_t5.map(tokenize_t5_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DI_FT_t5_base_model = T5ForConditionalGeneration.from_pretrained('DI_FT_GPTQRA_diffalg/a2c_50ft_rouge/model').to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question(answer):\n",
    "    DI_FT_t5_base_model.eval()  \n",
    "    input_ids = t5_tokenizer.encode(\"answer: \" + answer, return_tensors=\"pt\").to(device)\n",
    "    outputs = DI_FT_t5_base_model.generate(input_ids, num_beams=5, early_stopping=True)\n",
    "    question = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "samples = val_dataset_t5.shuffle(seed=42).select(range(5))  \n",
    "\n",
    "\n",
    "for example in samples:\n",
    "    generated_question = generate_question(example['input_text'].replace(\"answer: \", \"\"))\n",
    "    print(f\"Answer: {example['input_text'].replace('answer: ', '')}\")\n",
    "    print(f\"Generated Question: {generated_question}\")\n",
    "    print(f\"Actual Question: {example['target_text']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "samples = val_dataset_t5  \n",
    "res = []\n",
    "\n",
    "with open('Ans2Seq/compare_work/DI_FT_rouge_GTPRQA_gen_0_2_dffalg_a2c_30e.txt', 'a') as file:\n",
    "    for example in tqdm(samples):\n",
    "        generated_question = generate_question(example['input_text'].replace(\"answer: \", \"\"))\n",
    "        res.append(generated_question.replace(\"enquiry: \", \"\"))\n",
    "        file.write((generated_question.replace(\"enquiry: \", \"\")+'\\n'))\n",
    "        # print(f\"Answer: {example['input_text'].replace('answer: ', '')}\")\n",
    "        # print(f\"Generated Question: {generated_question}\")\n",
    "        # print(f\"Actual Question: {example['target_text']}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bert_score import score\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def Calmetic(references:list[list[str]], predictions:list[str]):\n",
    "    '''\n",
    "    Input format:\n",
    "\n",
    "    predictions = [\n",
    "        \"What is the capital of France?\",\n",
    "        \"Who wrote the book?\",\n",
    "        \"What is the largest planet?\"\n",
    "    ]\n",
    "\n",
    "    references = [\n",
    "        [\"What is the capital city of France?\"],\n",
    "        [\"Who is the author of the book?\"],\n",
    "        [\"Which planet is the largest in the solar system?\"]\n",
    "    ]\n",
    "    '''\n",
    "\n",
    "    # # Load BLEU scorer\n",
    "    # bleu_metric = load_metric(\"bleu\")\n",
    "\n",
    "    # # Calculate BLEU score\n",
    "    predictions_tokenized = [word_tokenize(pred) for pred in predictions]\n",
    "    references_tokenized = [[word_tokenize(refs[0])] for refs in references]\n",
    "    # B_S = {}\n",
    "    # for n in range(1, 5):\n",
    "    #     bleu_metric.add_batch(predictions=predictions_tokenized, references=references_tokenized)\n",
    "    #     results = bleu_metric.compute(max_order=n)\n",
    "    #     B_S[f\"BLEU-{n}\"] = results\n",
    "    bleu_metric = evaluate.load(\"bleu\")\n",
    "    B_S = bleu_metric.compute(predictions=predictions, references=references,tokenizer=word_tokenize)\n",
    "    for i,n in enumerate(B_S['precisions']):\n",
    "        print(f\"BLEU-{i+1} score: {n:.5f}\")\n",
    "        \n",
    "\n",
    "\n",
    "    # Load ROUGE scorer\n",
    "    rouge_metric = load_metric(\"rouge\")\n",
    "    '''\n",
    "    ROUGE-1: Measures unigram matches between generated text and reference text.\n",
    "    ROUGE-2: Measures bigram matches between generated text and reference text.\n",
    "    ROUGE-L: Measures the longest common subsequence (LCS) between generated text and reference text.\n",
    "    ROUGE-Lsum: A variant based on LCS, specifically designed for evaluating long texts.\n",
    "    '''\n",
    "    # Calculate ROUGE scores\n",
    "    rouge_results = rouge_metric.compute(predictions=predictions, references=references)\n",
    "    rouge1_mid_f1 = rouge_results['rouge1'][1][2]\n",
    "    rouge2_mid_f1 = rouge_results['rouge2'][1][2]\n",
    "    rougeL_mid_f1 = rouge_results['rougeL'][1][2]\n",
    "    rougeLsum_mid_f1 = rouge_results['rougeLsum'][1][2]\n",
    "    print(f\"ROUGE-1 F1 score: {rouge1_mid_f1:.5f}\")\n",
    "    print(f\"ROUGE-2 F1 score: {rouge2_mid_f1:.5f}\")\n",
    "    print(f\"ROUGE-L F1 score: {rougeL_mid_f1:.5f}\")\n",
    "    print(f\"ROUGE-Lsum F1 score: {rougeLsum_mid_f1:.5f}\")\n",
    "\n",
    "    # Calculate METEOR score\n",
    "    meteor_scores = [meteor_score(references=refs, hypothesis=pred) for pred, refs in zip(predictions_tokenized, references_tokenized)]\n",
    "    average_meteor_score = sum(meteor_scores) / len(meteor_scores)\n",
    "    print(f\"Average METEOR score: {average_meteor_score:.5f}\")\n",
    "\n",
    "    # Calculate BERTScore\n",
    "    P, R, F1 = score(predictions, [ref[0] for ref in references], lang=\"en\", verbose=False)\n",
    "    average_bert_score = F1.mean().item()\n",
    "    print(f\"Average BERTScore F1: {average_bert_score:.5f}\")\n",
    "\n",
    "    return {\n",
    "        \"BLEU\":B_S,\n",
    "        \"ROUGE\":rouge_results,\n",
    "        \"METERO\":meteor_scores,\n",
    "        \"BERTScore\":{\"Precision\":P,\"Recall\":R,\"F1\":F1},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Ans2Seq/compare_work/DI_FT_rouge_GTPRQA_gen_0_2_dffalg_a2c_30e.txt', 'r') as file:\n",
    "    content = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = [ [i.replace('enquiry: ',\"\")] for i in val_dataset_t5['target_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = Calmetic(references=refs,predictions=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res['BLEU']['precisions'])\n",
    "print(res['ROUGE']['rougeL'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity as torch_cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')#SentenceTransformer(\"bert-base-uncased\") \n",
    "\n",
    "reference_texts_ = [ i.replace('enquiry: ',\"\") for i in val_dataset_t5['target_text'] ]\n",
    "embeddings1 = sentence_model.encode(content, convert_to_tensor=True)\n",
    "embeddings2 = sentence_model.encode(reference_texts_, convert_to_tensor=True)\n",
    "\n",
    "cosine_scores_2 = util.pytorch_cos_sim(embeddings1, embeddings2)   \n",
    "\n",
    "\n",
    "print(f\"Average Cosine Similarity: {cosine_scores_2.diagonal().mean()}\")\n",
    "print(f\"Biggest Cosine Similarity: {cosine_scores_2.diagonal().max()}\")\n",
    "print(f\"Middle Cosine Similarity: {cosine_scores_2.diagonal().median()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL4LM2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
