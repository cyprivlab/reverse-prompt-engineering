{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset and re-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T10:00:39.285937Z",
     "start_time": "2025-05-07T10:00:39.283576Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset,load_from_disk,Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T10:12:45.124847Z",
     "start_time": "2025-05-07T10:12:44.997877Z"
    }
   },
   "outputs": [],
   "source": [
    "# 导入训练与测试数据集\n",
    "# train_dataset_df = pd.read_parquet('datasets/alpaca-0.2-train') # datasets/alpaca-0.2-train\n",
    "# test_dataset_df = pd.read_parquet('datasets/alpaca-0.2-test')  # datasets/alpaca-0.2-test\n",
    "datasets_name = 'gptrqa' # 'alpaca'/'gptrqa'\n",
    "\n",
    "if datasets_name == 'alpaca':\n",
    "    train_dataset_df = load_from_disk('datasets/alpaca-0.2-train') # datasets/alpaca-0.2-train\n",
    "    test_dataset_df = load_from_disk('datasets/alpaca-0.2-test')   # datasets/alpaca-0.2-test\n",
    "else:\n",
    "    train_dataset_df = load_from_disk('datasets/GPTRQA-train') # datasets/alpaca-0.2-train\n",
    "    test_dataset_df = load_from_disk('datasets/GPTRQA-test')   # datasets/alpaca-0.2-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查数据集头10条数据的情况\n",
    "pd.DataFrame(test_dataset_df).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DI-t5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主要工作部分，就是看划分出来的数据集是否会影响模型的输出（想要追求的结果：划分不太会影响结果）\n",
    "split_size = 0.2 #比例为0.2-0.5\n",
    "train_test_split_t5 = train_dataset_df.train_test_split(test_size=split_size, seed=42) #seed保证可复现\n",
    "train_dataset_t5 = train_test_split_t5['train']\n",
    "val_dataset_t5 = train_test_split_t5['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_t5.save_to_disk(dataset_path=f'ablation/split_dataset/{datasets_name}/{datasets_name}_{split_size}-train')\n",
    "val_dataset_t5.save_to_disk(dataset_path=f'ablation/split_dataset/{datasets_name}/{datasets_name}_{split_size}-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化tokenizer\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "def tokenize_t5_function(examples):\n",
    "    model_inputs = t5_tokenizer(examples['input_text'], padding=\"max_length\", truncation=True)\n",
    "    labels = t5_tokenizer(examples['target_text'], padding=\"max_length\", truncation=True)\n",
    "    print(labels)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train_dataset_t5 = train_dataset_t5.map(tokenize_t5_function, batched=True)\n",
    "tokenized_val_dataset_t5 = val_dataset_t5.map(tokenize_t5_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型到GPU\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path='ablation/DI/gptrqa/gptrqa_DI_t5_small_0.2_25e'\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(model_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "num_epoches = 25\n",
    "\n",
    "training_args_t5 = TrainingArguments(\n",
    "    output_dir= f'ablation/split_train_model/{datasets_name}_{split_size}/DI',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=num_epoches,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer_t5 = Trainer(\n",
    "    model=t5_model,\n",
    "    args=training_args_t5,\n",
    "    train_dataset=tokenized_train_dataset_t5,\n",
    "    eval_dataset=tokenized_val_dataset_t5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始训练\n",
    "trainer_t5.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "trainer_t5.save_model(f'ablation/DI/{datasets_name}/{datasets_name}_DI_t5_small_{split_size}_{num_epoches}e') \n",
    "t5_tokenizer.save_pretrained(f'ablation/DI/{datasets_name}/{datasets_name}_DI_t5_small_{split_size}_{num_epoches}e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question(answer):\n",
    "    t5_model.eval()  # 将模型设置为评估模式\n",
    "    input_ids = t5_tokenizer.encode(\"answer: \" + answer, return_tensors=\"pt\").to(device)\n",
    "    outputs = t5_model.generate(input_ids, num_beams=5, early_stopping=True)\n",
    "    question = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据当前要训练的调整全局模型命名以及模型预测结果文件命名\n",
    "DI_generation_texts_pth = f'ablation/GenText/{datasets_name}/DI_{datasets_name}_gen_{split_size}.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# 获取几个样本\n",
    "samples = test_dataset_df  \n",
    "res = []\n",
    "# 生成问题并比较\n",
    "with open(DI_generation_texts_pth, 'w') as file:\n",
    "    for example in tqdm(samples):\n",
    "        generated_question = generate_question(example['input_text'])\n",
    "        res.append(generated_question.replace(\"enquiry: \", \"\"))\n",
    "        file.write((generated_question.replace(\"enquiry: \", \"\")+'\\n'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bert_score import score\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def Calmetic(references:list[list[str]], predictions:list[str]):\n",
    "    '''\n",
    "    Input format:\n",
    "\n",
    "    predictions = [\n",
    "        \"What is the capital of France?\",\n",
    "        \"Who wrote the book?\",\n",
    "        \"What is the largest planet?\"\n",
    "    ]\n",
    "\n",
    "    references = [\n",
    "        [\"What is the capital city of France?\"],\n",
    "        [\"Who is the author of the book?\"],\n",
    "        [\"Which planet is the largest in the solar system?\"]\n",
    "    ]\n",
    "    '''\n",
    "\n",
    "    # # 加载 BLEU 评分器\n",
    "    # bleu_metric = load_metric(\"bleu\")\n",
    "\n",
    "    # # 计算 BLEU 分数\n",
    "    predictions_tokenized = [word_tokenize(pred) for pred in predictions]\n",
    "    references_tokenized = [[word_tokenize(refs[0])] for refs in references]\n",
    "    # B_S = {}\n",
    "    # for n in range(1, 5):\n",
    "    #     bleu_metric.add_batch(predictions=predictions_tokenized, references=references_tokenized)\n",
    "    #     results = bleu_metric.compute(max_order=n)\n",
    "    #     B_S[f\"BLEU-{n}\"] = results\n",
    "    bleu_metric = evaluate.load(\"bleu\")\n",
    "    B_S = bleu_metric.compute(predictions=predictions, references=references,tokenizer=word_tokenize)\n",
    "    for i,n in enumerate(B_S['precisions']):\n",
    "        print(f\"BLEU-{i+1} score: {n:.5f}\")\n",
    "        \n",
    "\n",
    "\n",
    "    # 加载 ROUGE 评分器\n",
    "    rouge_metric = load_metric(\"rouge\")\n",
    "    '''\n",
    "    ROUGE-1: 衡量生成文本和参考文本之间的 unigram 匹配。\n",
    "    ROUGE-2: 衡量生成文本和参考文本之间的 bigram 匹配。\n",
    "    ROUGE-L: 衡量生成文本和参考文本之间的最长公共子序列(LCS)。\n",
    "    ROUGE-Lsum: 基于 LCS 的一个变体，专门用于长文本的评估。\n",
    "    '''\n",
    "    # 计算 ROUGE 分数\n",
    "    rouge_results = rouge_metric.compute(predictions=predictions, references=references)\n",
    "    rouge1_mid_f1 = rouge_results['rouge1'][1][2]\n",
    "    rouge2_mid_f1 = rouge_results['rouge2'][1][2]\n",
    "    rougeL_mid_f1 = rouge_results['rougeL'][1][2]\n",
    "    rougeLsum_mid_f1 = rouge_results['rougeLsum'][1][2]\n",
    "    print(f\"ROUGE-1 F1 score: {rouge1_mid_f1:.5f}\")\n",
    "    print(f\"ROUGE-2 F1 score: {rouge2_mid_f1:.5f}\")\n",
    "    print(f\"ROUGE-L F1 score: {rougeL_mid_f1:.5f}\")\n",
    "    print(f\"ROUGE-Lsum F1 score: {rougeLsum_mid_f1:.5f}\")\n",
    "\n",
    "    # 计算 METEOR 分数\n",
    "    meteor_scores = [meteor_score(references=refs, hypothesis=pred) for pred, refs in zip(predictions_tokenized, references_tokenized)]\n",
    "    average_meteor_score = sum(meteor_scores) / len(meteor_scores)\n",
    "    print(f\"Average METEOR score: {average_meteor_score:.5f}\")\n",
    "\n",
    "    # 计算 BERTScore 分数\n",
    "\n",
    "    P, R, F1 = score(predictions, [ref[0] for ref in references], lang=\"en\", verbose=False)\n",
    "    average_bert_score = F1.mean().item()\n",
    "    print(f\"Average BERTScore F1: {average_bert_score:.5f}\")\n",
    "\n",
    "    return {\n",
    "        \"BLEU\":B_S,\n",
    "        \"ROUGE\":rouge_results,\n",
    "        \"METERO\":meteor_scores,\n",
    "        \"BERTScore\":{\"Precision\":P,\"Recall\":R,\"F1\":F1},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The path you save the DI_generation_text: ', DI_generation_texts_pth)\n",
    "with open(DI_generation_texts_pth, 'r') as file:\n",
    "    content = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = [ [i.replace('enquiry: ',\"\")] for i in test_dataset_df['target_text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = Calmetic(references=refs,predictions=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两个重要数值\n",
    "print(res['BLEU'])\n",
    "print(res['ROUGE']['rougeLsum'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity as torch_cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')  #SentenceTransformer(\"bert-base-uncased\") \n",
    "\n",
    "reference_texts_ = [ i.replace('enquiry: ',\"\") for i in test_dataset_df['target_text'] ]\n",
    "embeddings1 = sentence_model.encode(content, convert_to_tensor=True)\n",
    "embeddings2 = sentence_model.encode(reference_texts_, convert_to_tensor=True)\n",
    "\n",
    "cosine_scores_2 = util.pytorch_cos_sim(embeddings1, embeddings2)   #[52002,52002]维度的矩阵，对角线上的值为对应文本的余弦相似度\n",
    "\n",
    "# 输出余弦相似度的值\n",
    "print(f\"Average Cosine Similarity: {cosine_scores_2.diagonal().mean()}\")\n",
    "print(f\"Biggest Cosine Similarity: {cosine_scores_2.diagonal().max()}\")\n",
    "print(f\"Middle Cosine Similarity: {cosine_scores_2.diagonal().median()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DI+FT（需要用到RL4LM）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "train_dataset_t5 = load_from_disk('')\n",
    "val_dataset_t5 = load_from_disk('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(val_dataset_t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "def tokenize_t5_function(examples):\n",
    "    model_inputs = t5_tokenizer(examples['input_text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    labels = t5_tokenizer(examples['target_text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "#tokenized_train_dataset_t5 = train_dataset_t5.map(tokenize_t5_function, batched=True)\n",
    "tokenized_val_dataset_t5 = val_dataset_t5.map(tokenize_t5_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DI_FT_t5_base_model = T5ForConditionalGeneration.from_pretrained('++').to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question(answer):\n",
    "    DI_FT_t5_base_model.eval()  # 将模型设置为评估模式\n",
    "    input_ids = t5_tokenizer.encode(\"answer: \" + answer, return_tensors=\"pt\").to(device)\n",
    "    outputs = DI_FT_t5_base_model.generate(input_ids, num_beams=10,max_length=250,temperature=100,top_k=200)\n",
    "    question = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取几个样本\n",
    "samples = val_dataset_t5.shuffle(seed=42).select(range(5))  # 随机选择5个样本\n",
    "\n",
    "# 生成问题并比较\n",
    "for example in samples:\n",
    "    generated_question = generate_question(example['input_text'].replace(\"answer: \", \"\"))\n",
    "    print(f\"Answer: {example['input_text'].replace('answer: ', '')}\")\n",
    "    print(f\"Generated Question: {generated_question}\")\n",
    "    print(f\"Actual Question: {example['target_text']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openworld_responses = [\"Based on your symptoms, it sounds like you may have a fracture in your hand.\",\n",
    "\"It seems like you may have a foreign body stuck in your nose causing those symptoms. We will need to take a look and remove it if necessary.\",\n",
    "\"Based on your symptoms, it's possible that you could have esophageal cancer. Fatigue is a common symptom of this disease. We'll need to run some tests to confirm the diagnosis.\",\n",
    "\"You will need radiographic imaging of your shoulder, including a plain x-ray to see the extent of the injury. We may also need to suture the wound, perform a complete blood count, and provide intravenous fluid replacement. Additionally, we will need to manage wound care and perform kidney function tests to monitor renal function.\",\n",
    "\"Based on your symptoms, you may have a corneal abrasion, which is a scratch on the clear, protective layer on the front of your eye. Have you had anything come in contact with your eye recently?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取几个样本\n",
    "samples = openworld_responses  # 随机选择5个样本\n",
    "\n",
    "# 生成问题并比较\n",
    "for example in samples:\n",
    "    generated_question = generate_question(example)\n",
    "    print(f\"Answer: {example}\")\n",
    "    print(f\"Generated Question: {generated_question}\")\n",
    "    print(f\"Actual Question: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# 获取几个样本\n",
    "samples = val_dataset_t5  # 随机选择5个样本\n",
    "res = []\n",
    "# 生成问题并比较\n",
    "with open('DI_FT_rouge_meddata_gen_0_2_ppo_25e.txt', 'a') as file:\n",
    "    for example in tqdm(samples):\n",
    "        generated_question = generate_question(example['input_text'].replace(\"answer: \", \"\"))\n",
    "        res.append(generated_question.replace(\"enquiry: \", \"\"))\n",
    "        file.write((generated_question.replace(\"enquiry: \", \"\")+'\\n'))\n",
    "        # print(f\"Answer: {example['input_text'].replace('answer: ', '')}\")\n",
    "        # print(f\"Generated Question: {generated_question}\")\n",
    "        # print(f\"Actual Question: {example['target_text']}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bert_score import score\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def Calmetic(references:list[list[str]], predictions:list[str]):\n",
    "    '''\n",
    "    Input format:\n",
    "\n",
    "    predictions = [\n",
    "        \"What is the capital of France?\",\n",
    "        \"Who wrote the book?\",\n",
    "        \"What is the largest planet?\"\n",
    "    ]\n",
    "\n",
    "    references = [\n",
    "        [\"What is the capital city of France?\"],\n",
    "        [\"Who is the author of the book?\"],\n",
    "        [\"Which planet is the largest in the solar system?\"]\n",
    "    ]\n",
    "    '''\n",
    "\n",
    "    # # 加载 BLEU 评分器\n",
    "    # bleu_metric = load_metric(\"bleu\")\n",
    "\n",
    "    # # 计算 BLEU 分数\n",
    "    predictions_tokenized = [word_tokenize(pred) for pred in predictions]\n",
    "    references_tokenized = [[word_tokenize(refs[0])] for refs in references]\n",
    "    # B_S = {}\n",
    "    # for n in range(1, 5):\n",
    "    #     bleu_metric.add_batch(predictions=predictions_tokenized, references=references_tokenized)\n",
    "    #     results = bleu_metric.compute(max_order=n)\n",
    "    #     B_S[f\"BLEU-{n}\"] = results\n",
    "    bleu_metric = evaluate.load(\"bleu\")\n",
    "    B_S = bleu_metric.compute(predictions=predictions, references=references,tokenizer=word_tokenize)\n",
    "    for i,n in enumerate(B_S['precisions']):\n",
    "        print(f\"BLEU-{i+1} score: {n:.5f}\")\n",
    "        \n",
    "\n",
    "\n",
    "    # 加载 ROUGE 评分器\n",
    "    rouge_metric = load_metric(\"rouge\")\n",
    "    '''\n",
    "    ROUGE-1: 衡量生成文本和参考文本之间的 unigram 匹配。\n",
    "    ROUGE-2: 衡量生成文本和参考文本之间的 bigram 匹配。\n",
    "    ROUGE-L: 衡量生成文本和参考文本之间的最长公共子序列(LCS)。\n",
    "    ROUGE-Lsum: 基于 LCS 的一个变体，专门用于长文本的评估。\n",
    "    '''\n",
    "    # 计算 ROUGE 分数\n",
    "    rouge_results = rouge_metric.compute(predictions=predictions, references=references)\n",
    "    rouge1_mid_f1 = rouge_results['rouge1'][1][2]\n",
    "    rouge2_mid_f1 = rouge_results['rouge2'][1][2]\n",
    "    rougeL_mid_f1 = rouge_results['rougeL'][1][2]\n",
    "    rougeLsum_mid_f1 = rouge_results['rougeLsum'][1][2]\n",
    "    print(f\"ROUGE-1 F1 score: {rouge1_mid_f1:.5f}\")\n",
    "    print(f\"ROUGE-2 F1 score: {rouge2_mid_f1:.5f}\")\n",
    "    print(f\"ROUGE-L F1 score: {rougeL_mid_f1:.5f}\")\n",
    "    print(f\"ROUGE-Lsum F1 score: {rougeLsum_mid_f1:.5f}\")\n",
    "\n",
    "    # 计算 METEOR 分数\n",
    "    meteor_scores = [meteor_score(references=refs, hypothesis=pred) for pred, refs in zip(predictions_tokenized, references_tokenized)]\n",
    "    average_meteor_score = sum(meteor_scores) / len(meteor_scores)\n",
    "    print(f\"Average METEOR score: {average_meteor_score:.5f}\")\n",
    "\n",
    "    # 计算 BERTScore 分数\n",
    "\n",
    "    P, R, F1 = score(predictions, [ref[0] for ref in references], lang=\"en\", verbose=False)\n",
    "    average_bert_score = F1.mean().item()\n",
    "    print(f\"Average BERTScore F1: {average_bert_score:.5f}\")\n",
    "\n",
    "    return {\n",
    "        \"BLEU\":B_S,\n",
    "        \"ROUGE\":rouge_results,\n",
    "        \"METERO\":meteor_scores,\n",
    "        \"BERTScore\":{\"Precision\":P,\"Recall\":R,\"F1\":F1},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('real_world/DI_FT_rouge_meddata_gen_0_2_ppo_25e.txt', 'r') as file:\n",
    "    content = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = [ [i.replace('enquiry: ',\"\")] for i in val_dataset_t5['target_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = Calmetic(references=refs,predictions=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res['BLEU']['precisions'])\n",
    "print(res['ROUGE']['rougeL'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity as torch_cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')#SentenceTransformer(\"bert-base-uncased\") \n",
    "\n",
    "reference_texts_ = [ i.replace('enquiry: ',\"\") for i in val_dataset_t5['target_text'] ]\n",
    "embeddings1 = sentence_model.encode(content, convert_to_tensor=True)\n",
    "embeddings2 = sentence_model.encode(reference_texts_, convert_to_tensor=True)\n",
    "\n",
    "cosine_scores_2 = util.pytorch_cos_sim(embeddings1, embeddings2)   #[52002,52002]维度的矩阵，对角线上的值为对应文本的余弦相似度\n",
    "\n",
    "# 输出余弦相似度的值\n",
    "print(f\"Average Cosine Similarity: {cosine_scores_2.diagonal().mean()}\")\n",
    "print(f\"Biggest Cosine Similarity: {cosine_scores_2.diagonal().max()}\")\n",
    "print(f\"Middle Cosine Similarity: {cosine_scores_2.diagonal().median()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL4LM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
